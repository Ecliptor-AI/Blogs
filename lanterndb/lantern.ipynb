{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Compliance Automation with Lantern and Ecliptor\n",
    "\n",
    "In the financial industry, businesses must ensure that every interaction with customers complies with strict regulations. Traditionally, this involves people manually reviewing conversations, which is slow and can lead to mistakes. In addition, as regulations change, this can be hard to maintain.\n",
    "\n",
    "Rule-based automations can help automate this, but most customer interaction data is unstructured, such as call transcripts. Furthermore, data formats like PDFs or images can be difficult to extract information from. This makes it difficult to build simple rules over the data we have for automation.\n",
    "\n",
    "In this article, we’ll build an application that, given a set of customer interactions, efficiently searches for relevant compliance context, enabling automated compliance checks when combined with LLMs.\n",
    "\n",
    "To do this, we’ll use Ecliptor to parse and process unstructured documents into structured formats. Ecliptor helps financial institutions process messy data so that they can use the data to build applications. We’ll store this data in Lantern Cloud — Lantern enables vector search and text search in Postgres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ingest compliance policy documents\n",
    "\n",
    "Financial services organizations have compliance policies across a multitude of documents to adhere to. To make use of these documents, we’ll transform the PDFs to Markdown using Ecliptor's Document Ingest API. This endpoint preserves table formatting and document structure.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "You can download sample documents detailing compliance acts and regulations from here: [Banking Compliance Regulations and Acts](https://www.aba.com/banking-topics/compliance/acts#sort=%40stitle%20ascending)\n",
    "\n",
    "### Make a call to Ecliptor's ingest endpoint\n",
    "\n",
    "In your application, make a request to Ecliptor's API a the link to the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ecliptor's PDF ingest endpoint\n",
    "api_url = \"https://api.ecliptor.com/ingest/pdf\"\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"url\": pdf_url\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(api_url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    result = response.json()\n",
    "    markdown_url = result.markdown_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting markdown file contains the information in the PDF — we can now process this text into chunks for vector search.\n",
    "\n",
    "In this article, we’ll use the \"EQUAL CREDIT OPPORTUNITY ACT\", accessible [here](https://www.ecfr.gov/current/title-12/chapter-X/part-1002). Download a sample of the generated markdown [from our github](https://www.notion.so/github)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create chunks for analysis\n",
    "\n",
    "Simply converting the documents into text isn’t enough for effective searching and comparison. These documents are often lengthy and cover multiple topics, making it difficult to extract the relevant subset of information.\n",
    "\n",
    "To address this, we break the text into smaller, meaningful sections — also referred to as chunks. One naive way to do this is to simply split text based on character count or sentence boundaries. However, this can leave out relevant context.\n",
    "\n",
    "Ecliptor’s Smart Chunking API generates semantically meaningful chunks by analyzing the structure of the document, and injecting additional relevant information from elsewhere in the text if necessary. This approach allows us to get the most relevant and sufficient information to answer questions.\n",
    "\n",
    "### Make a call to Ecliptor's chunking endpoint\n",
    "\n",
    "Pass the generated markdown file to Ecliptor’s chunking endpoint to receive a list of chunks to embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecliptor's chunking endpoint, which accepts markdown files\n",
    "api_url = \"https://api.ecliptor.com/chunk\"\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"url\": markdown_url\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(api_url, json=payload)\n",
    "\n",
    "chunks = []\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    result = response.json()\n",
    "    chunks = response.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the API call is completed, you will have a list of roughly uniformly sized chunks which can be embedded using any embedding model.\n",
    "\n",
    "Take a look at a sample of generated chunks in `Target-Corporation-Reports-Second-Quarter-Earnings-Chunks.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store the chunks and generate embeddings\n",
    "\n",
    "Next, we’ll use Lantern to store the chunks and index them for fast retrieval. You can sign up for a free database at Lantern Cloud.\n",
    "\n",
    "### Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"postgresql://postgres:postgres@localhost:5432/bank\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = f\"CREATE TABLE compliance_documents (id integer, chunk text, vector real[]);\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings using Open AI's embeddings model\n",
    "\n",
    "Lantern can automatically generate embeddings of our data. To do this, you can simply enable an embedding generation job.\n",
    "\n",
    "This can be done in the Lantern Cloud dashboard, or with SQL inside your database. We use the Python client below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_embedding_job_query = \"\"\"\n",
    "\tSET lantern_extras.openai_token = 'API_TOKEN';\n",
    "\tSELECT add_embedding_job(\n",
    "\t    'compliance_documents',         -- Name of the table\n",
    "\t    'chunk',                        -- Source column for embeddings\n",
    "\t    'vector',                       -- Destination column for embeddings\n",
    "\t    'openai/text-embedding-3-large' -- Embedding model to use\n",
    "\t);\n",
    "\"\"\"\n",
    "cur.execute(add_embedding_job_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the embedding job service can be found [here](https://lantern.dev/docs/lantern-extras/daemon).\n",
    "\n",
    "To see what embeddings were generated on your data, you can run the SQL query below.\n",
    "\n",
    "```\n",
    "SELECT vector FROM compliance_documents;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Indexes for efficient search\n",
    "\n",
    "We now have the contexts of our compliance documents and the corresponding generated embeddings stored in the `compliance_documents` table. The next step is to create indexes over the data we want to search, to enable faster search over a large number of documents.\n",
    "\n",
    "We’ll create an HNSW index over our vectors with the cosine similarity distance function, and a BM25 index over our chunks.\n",
    "\n",
    "```\n",
    "cursor.execute(f\"CREATE INDEX ON {TABLE_NAME} USING lantern_hnsw (vector cos_dist);\")\n",
    "conn.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement our compliance check application.\n",
    "\n",
    "## Step 4: Build an application to check customer interactions for compliance risks\n",
    "\n",
    "Finally, we’ll build an application to check customer chat logs for compliance with regulations.\n",
    "\n",
    "We’ll follow the following steps:\n",
    "\n",
    "1. Embedding: We will generate vectors for each customer support chat message.\n",
    "2. Search: We will use Lantern’s vector search to find the most relevant compliance chunks for each chat message.\n",
    "3. LLM: We will input the chat message and the relevant compliance text into an LLM to determine compliance, flagging potential violations.\n",
    "\n",
    "### Chat interactions data set\n",
    "\n",
    "We’ll use a synthetically generated dataset of customer support chats.\n",
    "\n",
    "In these chats, clients are asking the customer support agents questions about the bank’s credit assessment process.\n",
    "\n",
    "The downloadable csv can be found here: [Bank Customer Support](https://www.notion.so/github)\n",
    "\n",
    "The dataset has the headers `id` (int), `speaker_role` (string), `text` (string), `compliant` (bool). We will use entries in the `text` column as input queries and use the `compliance` column as ground truth for evlauating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download the data set\n",
    "df = pd.read_csv(\"bank_compliance_qa.csv\")\n",
    "\n",
    "# Grab all the queries\n",
    "query_list = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create queries from each of the texts into the index\n",
    "\n",
    "First generate embeddings for all of the queries. We will use the same embeddings model used to embed the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client using your API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def embed_text(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-3-large\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Embed all queries and store\n",
    "embedded_queries = [embed_text(query) for query in query_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform lookups within the vector database using the embedded queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "# Set once at the beginning of a session, to force postgres to use our index\n",
    "cursor.execute(\"SET enable_seqscan = false;\")\n",
    "\n",
    "similar_chunks = {}\n",
    "\n",
    "\n",
    "for embedded_query in embedded_queries:\n",
    "    cursor.execute(\"SELECT id, cos_dist(vector, %s) AS dist, vector FROM small_world ORDER BY vector <-> %s LIMIT 3;\", (embedded_query, embedded_query))\n",
    "    query_index = embedded_queries.index(embedded_query)\n",
    "    original_query = query_list[query_index]\n",
    "    \n",
    "    # Fetch the results\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Store the chunks in the dictionary\n",
    "    similar_chunks[original_query] = [\n",
    "        {\n",
    "            'id': row[0],\n",
    "            'distance': row[1],\n",
    "            'vector': row[2]\n",
    "        } for row in results\n",
    "    ]\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag non-compliant responses\n",
    "\n",
    "Once we have found all the similar chunks, we have the information we need to be able to judge whether each chunk was adhering to compliance principles. We will use an LLM as a judge to flag possible non-compliance and return those responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def check_compliance(query: str, chunks: List[Dict[Any, Any]]) -> bool:\n",
    "\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Question: {query}\n",
    "\n",
    "    Relevant texts:\n",
    "    1. {chunks[0]['id']}\n",
    "    2. {chunks[1]['id']}\n",
    "    3. {chunks[2]['id']}\n",
    "\n",
    "    Based on the question and the relevant texts, determine if the response is compliant with ethical and legal standards and company policy.\n",
    "    Return only a boolean value: True if compliant, False if not compliant. Do not return any other descriptive text, only the one word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the API call\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a compliance checker. Respond with only 'True' or 'False'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Extract and return the boolean result\n",
    "    result = response.choices[0].message.content.strip().lower() == 'true'\n",
    "    return result\n",
    "\n",
    "# Check compliance for each query and its similar chunks\n",
    "compliance_results = {}\n",
    "for query, chunks in similar_chunks.items():\n",
    "    is_compliant = check_compliance(query, chunks)\n",
    "    compliance_results[query] = is_compliant\n",
    "\n",
    "# Print out non-compliant queries\n",
    "print(\"Non-compliant queries:\")\n",
    "for query, is_compliant in compliance_results.items():\n",
    "    if not is_compliant:\n",
    "        print(f\"- {query}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this post, we demonstrated a system for ensuring compliance in banking customer support using a custom dataset. We leveraged document understanding APIs from Ecliptor, data storage and search in Postgres with Lantern Cloud, and LLMs to automatically reason about compliance.\n",
    "\n",
    "## Interested in learning more?\n",
    "\n",
    "Lantern is building Postgres for AI applications. Learn more about how Lantern supports [vector search] and [text search] [at scale], or sign up for a free database at [Lantern Cloud].\n",
    "\n",
    "Ecliptor is currently in private beta for financial services companies. If you have complex documents and want to extract valuable insights for downstream applications like in this post, reach out to us at [nanki@ecliptor.ai](mailto:nanki@ecliptor.ai) or visit ecliptor.ai.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
